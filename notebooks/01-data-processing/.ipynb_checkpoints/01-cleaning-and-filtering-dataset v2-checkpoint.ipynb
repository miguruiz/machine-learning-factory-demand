{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILTERING THE DATASET:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along this script we will get from an initial list of products provided by our client, to a final list (as per the names and ids present within the real data), which will be used to filter our initial data in order to get a smaller, more manageable file.\n",
    "\n",
    "This process will be divided in two main steps:\n",
    "\n",
    "- Check the names in our list with the descriptions present in our data, analyze them and select a final list\n",
    "\n",
    "- Use this list to filter our data and store the resulting information in a more small and convenient file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING THE LIST OF PRODUCTS FOR THE ANALYSIS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After rearranging the data in a more convenient manner and doing some introductory analysis of the data, we now want to get down to work with our data.\n",
    "\n",
    "A list has been given to us of the 10 products that our clients found as more relevant to their business.\n",
    "\n",
    "What we want now is to check whether the names on the list correspond to certain uniques ids, or, as seen in the previous scripts, some conflict of unicity will arise between the id of our products and their descriptions.\n",
    "\n",
    "So, we are going to check our dataframe and select from it the ids and descriptions of our products that match the indications given in our clients list. With the lists (in reality, two dictionaries) of the ids and descriptions that match every product given to us, we will decide which are the more appropriate.\n",
    "\n",
    "Perhaps some guidance from our client would be needed at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miruiz/kschool-final-project/lib/python2.7/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "# Importing packages:\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import math\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the search path of the file, the name and the separator:\n",
    "\n",
    "file_path = \"../../data/01_raw/\"\n",
    "file_name = \"b2-transactions.csv\" #'b2-transactions_sample.csv' \n",
    "exit_path = \"../../data/02_intermediate/\"\n",
    "\n",
    "filtered_file_name=\"c1-filtered_transactions.csv\"\n",
    "\n",
    "sep=\";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the list of products provided by the client\n",
    "list_of_products=['croissant',\n",
    "                  'croissant petit',\n",
    "                  'tarta mousse 3 chocolates',\n",
    "                  'tarta de manzana 2º',\n",
    "                  'palmera de chocolate'\n",
    "                  'tarta opera',\n",
    "                  'postre fresas y mascarpone',\n",
    "                  'milhojas frambuesa 2º',\n",
    "                  'tortel',\n",
    "                  'baguette']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We import the dataframe:\n",
    "df=pd.read_csv(file_path+file_name, sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>description</th>\n",
       "      <th>order_date</th>\n",
       "      <th>section</th>\n",
       "      <th>store</th>\n",
       "      <th>units_ordered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16951106</th>\n",
       "      <td>458.0</td>\n",
       "      <td>S.HONORET CREMA-TRUFA 3º</td>\n",
       "      <td>9/3/2008 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>GoUP</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17584418</th>\n",
       "      <td>3301.0</td>\n",
       "      <td>CREMA DE ESPINACAS</td>\n",
       "      <td>7/1/2016 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>GoUP</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18271266</th>\n",
       "      <td>865.0</td>\n",
       "      <td>POSTRE OPERA</td>\n",
       "      <td>23/5/2010 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>GrUP</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513341</th>\n",
       "      <td>101.0</td>\n",
       "      <td>CROISSANT</td>\n",
       "      <td>29/5/2016 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>BmUP</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640037</th>\n",
       "      <td>502.0</td>\n",
       "      <td>SABLE CHOCOLATE</td>\n",
       "      <td>22/7/2012 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>VeUp</td>\n",
       "      <td>1,00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          product_id               description         order_date  section  \\\n",
       "16951106       458.0  S.HONORET CREMA-TRUFA 3º   9/3/2008 0:00:00        0   \n",
       "17584418      3301.0        CREMA DE ESPINACAS   7/1/2016 0:00:00        0   \n",
       "18271266       865.0              POSTRE OPERA  23/5/2010 0:00:00        0   \n",
       "1513341        101.0                 CROISSANT  29/5/2016 0:00:00        0   \n",
       "1640037        502.0           SABLE CHOCOLATE  22/7/2012 0:00:00        0   \n",
       "\n",
       "         store units_ordered  \n",
       "16951106  GoUP          0,00  \n",
       "17584418  GoUP          0,00  \n",
       "18271266  GrUP          0,00  \n",
       "1513341   BmUP          0,00  \n",
       "1640037   VeUp          1,00  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Normalizing and aggregating description names\n",
    "\n",
    "Unfortunately, there is no convention for the description and one id could \n",
    "\n",
    "1. Normalize descriptions as much as possible using:\n",
    "    - Regex expressions \n",
    "    - Basic NLP for spell-checking.\n",
    "2. Create a normalization file with the following structure:\n",
    "    - Unique Product_id and normalized description\n",
    "    - Flag to indicate if the product is part of the given list, or not.  \n",
    "3. Finally review the list manually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Normalizing description names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Null descriptions to 'no-description'\n",
    "df['description'].fillna('no-description', inplace = True)\n",
    "\n",
    "# Unique product descriptions\n",
    "df_descriptions_unique = pd.Series(df['description'].unique())\n",
    "\n",
    "# Most of the descriptions are in uppercase, however others are in lower:\n",
    "df_descriptions_normalized = df_descriptions_unique.str.lower()\n",
    "\n",
    "#replace non alfanumeric with space\n",
    "df_descriptions_normalized=df_descriptions_normalized.str.replace(r'[^0-9a-zA-Zº()ª:-]+', ' ') \n",
    "\n",
    "# We also notice that there are spacing issues at the begining, end of the description and between words:\n",
    "df_descriptions_normalized=df_descriptions_normalized.str.strip()\n",
    "\n",
    "# Remove multi-spacing. multi '-' and multi ':'\n",
    "df_descriptions_normalized=df_descriptions_normalized.str.replace(r' +', ' ') \n",
    "df_descriptions_normalized=df_descriptions_normalized.str.replace(r'-+', ' ') \n",
    "df_descriptions_normalized=df_descriptions_normalized.str.replace(r':+', ' ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desc_original</th>\n",
       "      <th>desc_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10857</th>\n",
       "      <td>Encargo ROLLITOS DE PRIMAVERA</td>\n",
       "      <td>encargo rollitos de primavera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29049</th>\n",
       "      <td>TARTA COMUNION 4º</td>\n",
       "      <td>tarta comunion 4º</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4567</th>\n",
       "      <td>TARTA TRES CHOCOLATES DE 16 RACIONES,</td>\n",
       "      <td>tarta tres chocolates de 16 raciones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42161</th>\n",
       "      <td>COCA DE CREMA</td>\n",
       "      <td>coca de crema</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11722</th>\n",
       "      <td>CORVINA EN SALSA VERDE</td>\n",
       "      <td>corvina en salsa verde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32399</th>\n",
       "      <td>Encargo  TARTA CREMA CATALANA Y FOTO CLIENTE</td>\n",
       "      <td>encargo tarta crema catalana y foto cliente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22171</th>\n",
       "      <td>TARTA SELVA NEGRA DE 50 RAC  CON EL FONDO EN V...</td>\n",
       "      <td>tarta selva negra de 50 rac con el fondo en ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7530</th>\n",
       "      <td>Encargo TARTA MOUSE TRES CHOCOLATES DEL 1 FELI...</td>\n",
       "      <td>encargo tarta mouse tres chocolates del 1 feli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18767</th>\n",
       "      <td>Encargo TARTA VIRUTA DE CHOCOLATE DEL 3 ,,FELI...</td>\n",
       "      <td>encargo tarta viruta de chocolate del 3 felici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38307</th>\n",
       "      <td>Caja de Mini Sándwiches de 5 rac. ( 8 son de e...</td>\n",
       "      <td>caja de mini s ndwiches de 5 rac ( 8 son de en...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           desc_original  \\\n",
       "10857                      Encargo ROLLITOS DE PRIMAVERA   \n",
       "29049                                  TARTA COMUNION 4º   \n",
       "4567               TARTA TRES CHOCOLATES DE 16 RACIONES,   \n",
       "42161                                     COCA DE CREMA    \n",
       "11722                             CORVINA EN SALSA VERDE   \n",
       "32399       Encargo  TARTA CREMA CATALANA Y FOTO CLIENTE   \n",
       "22171  TARTA SELVA NEGRA DE 50 RAC  CON EL FONDO EN V...   \n",
       "7530   Encargo TARTA MOUSE TRES CHOCOLATES DEL 1 FELI...   \n",
       "18767  Encargo TARTA VIRUTA DE CHOCOLATE DEL 3 ,,FELI...   \n",
       "38307  Caja de Mini Sándwiches de 5 rac. ( 8 son de e...   \n",
       "\n",
       "                                         desc_normalized  \n",
       "10857                      encargo rollitos de primavera  \n",
       "29049                                  tarta comunion 4º  \n",
       "4567                tarta tres chocolates de 16 raciones  \n",
       "42161                                      coca de crema  \n",
       "11722                             corvina en salsa verde  \n",
       "32399        encargo tarta crema catalana y foto cliente  \n",
       "22171  tarta selva negra de 50 rac con el fondo en ve...  \n",
       "7530   encargo tarta mouse tres chocolates del 1 feli...  \n",
       "18767  encargo tarta viruta de chocolate del 3 felici...  \n",
       "38307  caja de mini s ndwiches de 5 rac ( 8 son de en...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dict(desc_original = df_descriptions_unique, desc_normalized = df_descriptions_normalized)).sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets gets get our hands dirty and apply some maths to calculate string distnace and finish cleaning all those messy product descriptions... This is what we are going to do:\n",
    "\n",
    "1. Create a dataset with pastry products by parsing the bakery catalogues, and other pastry websites. (this was done manually, by converting the pdf catalogues to txt using an external web. THe resulting file is named productos.txt)\n",
    "\n",
    "2. Following the indications from: https://medium.com/@hdezfloresmiguelangel/implementando-un-corrector-ortogr%C3%A1fico-en-python-utilizando-la-distancia-de-levenshtein-498ec0dd1105 create an spell-checker based on the products.txt dataset and the Levenshtein distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('../../data/01_additional_data/productos.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tarta'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction(\"trta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cafe'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction(\"café\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! the it seems to work. Lets now apply it to our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_check (line):\n",
    "    \"Given a sentence, returns spell-checks word by word\"\n",
    "    if type(line) == str and len(line) > 0:\n",
    "        new = []\n",
    "        line = line.split(\" \")\n",
    "        for word in line:\n",
    "            if type(word) == str:\n",
    "                word = correction(word)\n",
    "            new.append(word)\n",
    "        return \" \".join(new)\n",
    "            \n",
    "    else:\n",
    "        return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAUTION! The following cell e may take a long time to process (5 hours): \n",
    "\n",
    "# spell-check word by word the dataset:\n",
    "df_descriptions_normalized = df_descriptions_normalized.apply(lambda line: spell_check(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now merge the normalized names back to the original file, and check how effective was this cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>description</th>\n",
       "      <th>order_date</th>\n",
       "      <th>section</th>\n",
       "      <th>store</th>\n",
       "      <th>units_ordered</th>\n",
       "      <th>desc_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16842805</th>\n",
       "      <td>9999.0</td>\n",
       "      <td>EncargoCARDO</td>\n",
       "      <td>20/12/2009 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>GoUP</td>\n",
       "      <td>0,00</td>\n",
       "      <td>encargocardo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28937085</th>\n",
       "      <td>709.0</td>\n",
       "      <td>CAPUCHINAS</td>\n",
       "      <td>20/12/2016 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>ZiUO</td>\n",
       "      <td>0,00</td>\n",
       "      <td>capuchina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22351725</th>\n",
       "      <td>9999.0</td>\n",
       "      <td>EncargoDIBUJO DE UNA FOTO TRAIDA POR UN CLIENTE</td>\n",
       "      <td>2/10/2016 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>MoUP</td>\n",
       "      <td>0,00</td>\n",
       "      <td>encargodibujo de una feo traida con un cliente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767947</th>\n",
       "      <td>245.0</td>\n",
       "      <td>Sandwiches mixtos</td>\n",
       "      <td>29/5/2015 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>VeUp</td>\n",
       "      <td>0,00</td>\n",
       "      <td>sandwich minutos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27761506</th>\n",
       "      <td>4510.0</td>\n",
       "      <td>TARTA FRESAS Y MASCARPONE</td>\n",
       "      <td>18/5/2015 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>ZiUO</td>\n",
       "      <td>0,00</td>\n",
       "      <td>tarta fresas y mascarpone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          product_id                                      description  \\\n",
       "16842805      9999.0                                     EncargoCARDO   \n",
       "28937085       709.0                                       CAPUCHINAS   \n",
       "22351725      9999.0  EncargoDIBUJO DE UNA FOTO TRAIDA POR UN CLIENTE   \n",
       "2767947        245.0                                Sandwiches mixtos   \n",
       "27761506      4510.0                        TARTA FRESAS Y MASCARPONE   \n",
       "\n",
       "                  order_date  section store units_ordered  \\\n",
       "16842805  20/12/2009 0:00:00        0  GoUP          0,00   \n",
       "28937085  20/12/2016 0:00:00        0  ZiUO          0,00   \n",
       "22351725   2/10/2016 0:00:00        0  MoUP          0,00   \n",
       "2767947    29/5/2015 0:00:00        0  VeUp          0,00   \n",
       "27761506   18/5/2015 0:00:00        0  ZiUO          0,00   \n",
       "\n",
       "                                         desc_normalized  \n",
       "16842805                                    encargocardo  \n",
       "28937085                                       capuchina  \n",
       "22351725  encargodibujo de una feo traida con un cliente  \n",
       "2767947                                 sandwich minutos  \n",
       "27761506                       tarta fresas y mascarpone  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_merge = pd.DataFrame(dict(description = df_descriptions_unique, desc_normalized = df_descriptions_normalized))\n",
    "\n",
    "df_with_normalized_descriptions_transactions = pd.merge(df, to_merge, how='left', on = 'description').sort_values(by='order_date')\n",
    "df_with_normalized_descriptions_transactions.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK - 'df' has the same size as 'df_with_normalized_descriptions' \n"
     ]
    }
   ],
   "source": [
    "#Control merge size:\n",
    "if (df.shape[0] == df_with_normalized_descriptions_transactions.shape[0] ): \n",
    "    test0 = \"OK - 'df' has the same size as 'df_with_normalized_descriptions_transactions' \"\n",
    "else:\n",
    "    test0 = \"ERROR - 'df' has NOT the same size as 'df_with_normalized_descriptions_transactions' \"\n",
    "print(test0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The product descritions were cleaned from 48517 unique names to 40024.\n"
     ]
    }
   ],
   "source": [
    "# Checking effectiveness of the data cleaning:\n",
    "unique_descriptions_raw = len(df['description'].unique())\n",
    "unique_descriptions_normalized = len(df_with_normalized_descriptions_transactions['desc_normalized'].unique())\n",
    "print('The product descritions were cleaned from {} unique names to {}.'.format(unique_descriptions_raw,unique_descriptions_normalized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not super effective..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the file to the intermiady folder\n",
    "output_path_df_with_normalized_descriptions_transactions = exit_path + 'data_with_normalized_names.csv'\n",
    "df_with_normalized_descriptions_transactions.to_csv(output_path_df_with_normalized_descriptions_transactions, index = False, sep = ';' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Identifying product descriptions that the client wants us to predict\n",
    "\n",
    "It is time to create the file that will be manually reviewed.\n",
    "\n",
    "- First, we compare the normalized descriptions with the list of products provided with the client, and suggest matches using the library fuzzywuzzy\n",
    "- Second, we will use the results from the other analysis.\n",
    "- third, we will manually evaluate if the results are good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Using the library fuzzywuzzy to compare the product normalized descriptions with the list of products provided by the client and suggest a match, or alternatively - \"match-not-found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized_desc_unique = pd.DataFrame(df_with_normalized_descriptions_transactions[\"desc_normalized\"].unique(), columns = ['desc_normalized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_match (line, options = list_of_products):\n",
    "    \"Returns product match if the the calculated difference between strings is greater than 80, 'match-not-found' otherwise\"\n",
    "    if not(line is None) and type(line)== str:\n",
    "        highest = process.extractOne(line,list_of_products)\n",
    "        if not(highest is None) and highest[1] >80:\n",
    "            return highest[0]\n",
    "        else:\n",
    "            return 'match-not-found'\n",
    "    else:\n",
    "        return 'match-not-found'\n",
    "\n",
    "# Applying matching function to all product normalized descriptions\n",
    "df_normalized_desc_unique[\"target_names_fuzzywuuzy\"] = df_normalized_desc_unique[\"desc_normalized\"].apply(lambda line: find_match(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now evaluate how effectively did we match the normalized descriptions with the list that the client provided us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desc_normalized</th>\n",
       "      <th>target_names_fuzzywuuzy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>mousse chocolate blanco</td>\n",
       "      <td>match-not-found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>postres mousse chocolate en vasito</td>\n",
       "      <td>match-not-found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>postres mousse frutas bosque</td>\n",
       "      <td>match-not-found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>postres mousse de limon</td>\n",
       "      <td>match-not-found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>petit yogur mousse praline</td>\n",
       "      <td>croissant petit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>mousse 3 chocolates 3</td>\n",
       "      <td>tarta mousse 3 chocolates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>mousse 3 chocolates 1</td>\n",
       "      <td>tarta mousse 3 chocolates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>mousse 3 chocolates 2</td>\n",
       "      <td>tarta mousse 3 chocolates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>postres mousse tres chocolates</td>\n",
       "      <td>match-not-found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>mousse de perigot</td>\n",
       "      <td>palmera de chocolatetarta opera</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        desc_normalized          target_names_fuzzywuuzy\n",
       "380             mousse chocolate blanco                  match-not-found\n",
       "417  postres mousse chocolate en vasito                  match-not-found\n",
       "419        postres mousse frutas bosque                  match-not-found\n",
       "482             postres mousse de limon                  match-not-found\n",
       "501          petit yogur mousse praline                  croissant petit\n",
       "569               mousse 3 chocolates 3        tarta mousse 3 chocolates\n",
       "570               mousse 3 chocolates 1        tarta mousse 3 chocolates\n",
       "571               mousse 3 chocolates 2        tarta mousse 3 chocolates\n",
       "572      postres mousse tres chocolates                  match-not-found\n",
       "617                   mousse de perigot  palmera de chocolatetarta opera"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets review the effectiveness filtering by 'mousse '. The expected result is that all 'mousse 3 chocolates' match\n",
    "df_normalized_desc_unique[df_normalized_desc_unique['desc_normalized'].str.contains('mousse')].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see... its not actually very good, lets try something different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Using the results from the other analysis\n",
    "\n",
    "Lets now use the results from the manual analysis to see how efective the measure was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this matching is performed at id level, \n",
    "# lets create a new dataset with unique product_id, descriptions, and evaluate it:\n",
    "df_normalized_id_desc_unique = df_with_normalized_descriptions_transactions[[\"product_id\",'desc_normalized']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_products_matches={100: 'croissant', \n",
    "                  101: 'croissant',\n",
    "                  102: 'croissant',\n",
    "                  103: 'croissant petit',\n",
    "                  9999: 'tarta mousse 3 chocolates', # almost only for order, creating a new id for this product is suggested\n",
    "                  462: 'tarta de manzana 2º',\n",
    "                  182: 'palmera de chocolate', # palmeras: 140\n",
    "                  414: 'tarta opera', # 9999, for order, mostly. If included, creating a new id for this product is suggested\n",
    "                  4511:'postre fresas y mascarpone',\n",
    "                  459: 'milhojas frambuesa 2º',\n",
    "                  112: 'tortel',\n",
    "                  115: 'baguette'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_names_a(product_id, dict_of_products_matches= dict_of_products_matches):\n",
    "    'Returns match if the product_id is found within the given dict or, otherise \"match-not-found\"'\n",
    "    if not(product_id is None) and not(math.isnan(product_id)) and int(product_id)  in dict_of_products_matches:\n",
    "        return dict_of_products_matches[int(product_id)]\n",
    "    else:\n",
    "        return 'match-not-found'\n",
    "    \n",
    "df_normalized_id_desc_unique['target_names_manual_analysis']=df_normalized_id_desc_unique[\"product_id\"].apply(lambda line: target_names_a(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now check how effective this was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>desc_normalized</th>\n",
       "      <th>target_names_manual_analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2498667</th>\n",
       "      <td>618.0</td>\n",
       "      <td>mousse chocolate blanco</td>\n",
       "      <td>match-not-found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479507</th>\n",
       "      <td>877.0</td>\n",
       "      <td>postres mousse chocolate en vasito</td>\n",
       "      <td>match-not-found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479509</th>\n",
       "      <td>879.0</td>\n",
       "      <td>postres mousse frutas bosque</td>\n",
       "      <td>match-not-found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13734845</th>\n",
       "      <td>821.0</td>\n",
       "      <td>postres mousse de limon</td>\n",
       "      <td>match-not-found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13734826</th>\n",
       "      <td>728.0</td>\n",
       "      <td>petit yogur mousse praline</td>\n",
       "      <td>match-not-found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25009569</th>\n",
       "      <td>453.0</td>\n",
       "      <td>mousse 3 chocolates 3</td>\n",
       "      <td>match-not-found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25009568</th>\n",
       "      <td>452.0</td>\n",
       "      <td>mousse 3 chocolates 1</td>\n",
       "      <td>match-not-found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25009567</th>\n",
       "      <td>451.0</td>\n",
       "      <td>mousse 3 chocolates 2</td>\n",
       "      <td>match-not-found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25009566</th>\n",
       "      <td>450.0</td>\n",
       "      <td>postres mousse tres chocolates</td>\n",
       "      <td>match-not-found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24921912</th>\n",
       "      <td>312.0</td>\n",
       "      <td>mousse de perigot</td>\n",
       "      <td>match-not-found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24921911</th>\n",
       "      <td>311.0</td>\n",
       "      <td>mousse de salmon</td>\n",
       "      <td>match-not-found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24921910</th>\n",
       "      <td>310.0</td>\n",
       "      <td>mousse de con opcion</td>\n",
       "      <td>match-not-found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24921909</th>\n",
       "      <td>309.0</td>\n",
       "      <td>mousse de con</td>\n",
       "      <td>match-not-found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21890758</th>\n",
       "      <td>425.0</td>\n",
       "      <td>postres mousse naranja</td>\n",
       "      <td>match-not-found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21879345</th>\n",
       "      <td>754.0</td>\n",
       "      <td>cartel de mousse de pistacho</td>\n",
       "      <td>match-not-found</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          product_id                     desc_normalized  \\\n",
       "2498667        618.0             mousse chocolate blanco   \n",
       "2479507        877.0  postres mousse chocolate en vasito   \n",
       "2479509        879.0        postres mousse frutas bosque   \n",
       "13734845       821.0             postres mousse de limon   \n",
       "13734826       728.0          petit yogur mousse praline   \n",
       "25009569       453.0               mousse 3 chocolates 3   \n",
       "25009568       452.0               mousse 3 chocolates 1   \n",
       "25009567       451.0               mousse 3 chocolates 2   \n",
       "25009566       450.0      postres mousse tres chocolates   \n",
       "24921912       312.0                   mousse de perigot   \n",
       "24921911       311.0                    mousse de salmon   \n",
       "24921910       310.0                mousse de con opcion   \n",
       "24921909       309.0                       mousse de con   \n",
       "21890758       425.0              postres mousse naranja   \n",
       "21879345       754.0        cartel de mousse de pistacho   \n",
       "\n",
       "         target_names_manual_analysis  \n",
       "2498667               match-not-found  \n",
       "2479507               match-not-found  \n",
       "2479509               match-not-found  \n",
       "13734845              match-not-found  \n",
       "13734826              match-not-found  \n",
       "25009569              match-not-found  \n",
       "25009568              match-not-found  \n",
       "25009567              match-not-found  \n",
       "25009566              match-not-found  \n",
       "24921912              match-not-found  \n",
       "24921911              match-not-found  \n",
       "24921910              match-not-found  \n",
       "24921909              match-not-found  \n",
       "21890758              match-not-found  \n",
       "21879345              match-not-found  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets review the effectiveness filtering by 'mousse '. The expected result is that all 'mousse 3 chocolates' match\n",
    "df_normalized_id_desc_unique[df_normalized_id_desc_unique['desc_normalized'].str.contains('mousse')].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, not very good, since most of the 'mousse 3 chocolates' are unmatched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that we need an better way to match the results. Lets try doing keywords filtering product by product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DELETE!\n",
    "output_path_df_with_normalized_descriptions_transactions = exit_path + 'data_with_normalized_names.csv'\n",
    "df_with_normalized_descriptions_transactions = pd.read_csv(output_path_df_with_normalized_descriptions_transactions, sep = ';' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Review Product by Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, lets create again a dataframe with unique descriptions\n",
    "unique_normalized_decriptions = df_with_normalized_descriptions_transactions[['product_id','desc_normalized']].drop_duplicates()\n",
    "\n",
    "#And a empty list to add all the unitary analysis. It will be use to concatenate results.\n",
    "list_of_dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions that we will use:\n",
    "\n",
    "def plot_count_per_id(df):\n",
    "    transactions_by_id = df.groupby(\"product_id\")['desc_normalized'].count()\n",
    "    transactions_by_id.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Matching: milhojas de frambuesa 2º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_milhojas (df):\n",
    "    'Filters the product descriptions of given dataset by \"milhojas\" and \"frambuesa 2\"'\n",
    "    milhojas = df[df['desc_normalized'].str.contains('milhojas')]\n",
    "    milhojas_frambuesa = milhojas[milhojas['desc_normalized'].str.contains('frambuesa 2')].copy()\n",
    "    return milhojas_frambuesa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>description</th>\n",
       "      <th>order_date</th>\n",
       "      <th>section</th>\n",
       "      <th>store</th>\n",
       "      <th>units_ordered</th>\n",
       "      <th>desc_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>414.0</td>\n",
       "      <td>TARTA  MILHOJAS   FRAMBUESA  2º</td>\n",
       "      <td>1/1/2008 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>VeUp</td>\n",
       "      <td>0,00</td>\n",
       "      <td>tarta milhojas frambuesa 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129</th>\n",
       "      <td>459.0</td>\n",
       "      <td>MILHOJAS   FRAMBUESA 2º</td>\n",
       "      <td>1/1/2008 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>MsUP</td>\n",
       "      <td>0,00</td>\n",
       "      <td>milhojas frambuesa 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1322</th>\n",
       "      <td>459.0</td>\n",
       "      <td>MILHOJAS   FRAMBUESA 2º</td>\n",
       "      <td>1/1/2008 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>ZiUO</td>\n",
       "      <td>0,00</td>\n",
       "      <td>milhojas frambuesa 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1358</th>\n",
       "      <td>459.0</td>\n",
       "      <td>MILHOJAS   FRAMBUESA 2º</td>\n",
       "      <td>1/1/2008 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>VeUp</td>\n",
       "      <td>2,00</td>\n",
       "      <td>milhojas frambuesa 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>414.0</td>\n",
       "      <td>TARTA  MILHOJAS   FRAMBUESA  2º</td>\n",
       "      <td>1/1/2008 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>ZiUO</td>\n",
       "      <td>0,00</td>\n",
       "      <td>tarta milhojas frambuesa 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      product_id                      description        order_date  section  \\\n",
       "336        414.0  TARTA  MILHOJAS   FRAMBUESA  2º  1/1/2008 0:00:00        0   \n",
       "1129       459.0          MILHOJAS   FRAMBUESA 2º  1/1/2008 0:00:00        0   \n",
       "1322       459.0          MILHOJAS   FRAMBUESA 2º  1/1/2008 0:00:00        0   \n",
       "1358       459.0          MILHOJAS   FRAMBUESA 2º  1/1/2008 0:00:00        0   \n",
       "1370       414.0  TARTA  MILHOJAS   FRAMBUESA  2º  1/1/2008 0:00:00        0   \n",
       "\n",
       "     store units_ordered             desc_normalized  \n",
       "336   VeUp          0,00  tarta milhojas frambuesa 2  \n",
       "1129  MsUP          0,00        milhojas frambuesa 2  \n",
       "1322  ZiUO          0,00        milhojas frambuesa 2  \n",
       "1358  VeUp          2,00        milhojas frambuesa 2  \n",
       "1370  ZiUO          0,00  tarta milhojas frambuesa 2  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the transactions dataset\n",
    "milhojas_frambuesa_transacciones=filter_milhojas(df_with_normalized_descriptions_transactions)\n",
    "milhojas_frambuesa_transacciones.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEqCAYAAADpvgyHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi40LCBodHRwOi8vbWF0cGxvdGxpYi5vcmcv7US4rQAAHuZJREFUeJzt3XuUnXV97/H3x0SolypBxogkaSJGF2A1SkQ8rS1KhYA9XKwo1Eq0LCMV2urqUbB6FiwrFuuy9tAiLpBIUEtQFEg1iDlU5bQ2SIDIHRkCSFIuMeGitV6In/PH8xvzZJzJ/Jg92XuH/Xmttdc8+/vcvrMnmc88d9kmIiKixlN63UBEROw8EhoREVEtoREREdUSGhERUS2hERER1RIaERFRLaERERHVEhoREVEtoREREdWm97qBqbbHHnt47ty5vW4jImKnct111/3Q9tBE0z3pQmPu3LmsWbOm121EROxUJN1bM112T0VERLWERkREVEtoREREtQlDQ9JSSQ9JurlVu1jS2vK6R9LaUp8r6b9b4z7dmmd/STdJGpZ0liSV+u6SVkm6s3ydUeoq0w1LulHSK6b+24+IiCeiZkvjAmBRu2D7LbYX2F4AfBn4Smv0XSPjbJ/Yqp8DvBOYX14jyzwVuMr2fOCq8h7gsNa0S8r8ERHRQxOGhu2rgc1jjStbC28GLtreMiTtCTzL9mo3T326EDiqjD4SWFaGl42qX+jGamC3spyIiOiRTo9pvAZ40Padrdo8STdI+rak15TaXsD61jTrSw1gpu37y/ADwMzWPPeNM09ERPRAp9dpHMe2Wxn3A3Nsb5K0P3CZpP1qF2bbkp7w82clLaHZhcWcOXOe6OwREVFp0lsakqYDbwQuHqnZ/pntTWX4OuAu4EXABmBWa/ZZpQbw4Mhup/L1oVLfAMweZ55t2D7X9kLbC4eGJrygMSIiJqmTLY0/AG63/avdTpKGgM22t0h6Ac1B7HW2N0t6TNKBwDXA8cA/ltlWAIuBM8vXy1v1kyUtB14FPNrajRXxpDT31K91vIx7znzDFHQSMbaaU24vAv4DeLGk9ZJOKKOO5dcPgP8ecGM5BfcS4ETbIwfR3w18Bhim2QK5otTPBF4v6U6aIDqz1FcC68r055X5IyKihybc0rB93Dj1t49R+zLNKbhjTb8GeMkY9U3AwWPUDZw0UX8REdE9uSI8IiKqJTQiIqJaQiMiIqolNCIiolpCIyIiqiU0IiKiWkIjIiKqJTQiIqJaQiMiIqolNCIiolpCIyIiqiU0IiKiWkIjIiKqJTQiIqJaQiMiIqolNCIiolpCIyIiqiU0IiKiWkIjIiKqJTQiIqJaQiMiIqpNGBqSlkp6SNLNrdrpkjZIWlteh7fGfUDSsKQ7JB3aqi8qtWFJp7bq8yRdU+oXS9ql1Hct74fL+LlT9U1HRMTk1GxpXAAsGqP+SdsLymslgKR9gWOB/co8n5I0TdI04GzgMGBf4LgyLcDHyrJeCDwMnFDqJwAPl/ony3QREdFDE4aG7auBzZXLOxJYbvtntu8GhoEDymvY9jrbPweWA0dKEvA64JIy/zLgqNaylpXhS4CDy/QREdEjnRzTOFnSjWX31YxS2wu4rzXN+lIbr/4c4BHbj4+qb7OsMv7RMn1ERPTIZEPjHGBvYAFwP/CJKetoEiQtkbRG0pqNGzf2spWIiCe1SYWG7Qdtb7H9S+A8mt1PABuA2a1JZ5XaePVNwG6Spo+qb7OsMv7ZZfqx+jnX9kLbC4eGhibzLUVERIVJhYakPVtvjwZGzqxaARxbznyaB8wHvgtcC8wvZ0rtQnOwfIVtA98E3lTmXwxc3lrW4jL8JuBfy/QREdEj0yeaQNJFwEHAHpLWA6cBB0laABi4B3gXgO1bJH0RuBV4HDjJ9paynJOBK4FpwFLbt5RVnAIsl/QR4Abg/FI/H/icpGGaA/HHdvzdRkRERyYMDdvHjVE+f4zayPRnAGeMUV8JrByjvo6tu7fa9Z8Cx0zUX0REdE+uCI+IiGoJjYiIqJbQiIiIagmNiIioltCIiIhqCY2IiKiW0IiIiGoJjYiIqJbQiIiIagmNiIioltCIiIhqCY2IiKiW0IiIiGoJjYiIqJbQiIiIagmNiIioltCIiIhqCY2IiKiW0IiIiGoJjYiIqJbQiIiIahOGhqSlkh6SdHOr9nFJt0u6UdKlknYr9bmS/lvS2vL6dGue/SXdJGlY0lmSVOq7S1ol6c7ydUapq0w3XNbziqn/9iMi4omo2dK4AFg0qrYKeIntlwLfBz7QGneX7QXldWKrfg7wTmB+eY0s81TgKtvzgavKe4DDWtMuKfNHREQPTRgatq8GNo+qfcP24+XtamDW9pYhaU/gWbZX2zZwIXBUGX0ksKwMLxtVv9CN1cBuZTkREdEjU3FM40+BK1rv50m6QdK3Jb2m1PYC1remWV9qADNt31+GHwBmtua5b5x5IiKiB6Z3MrOkDwKPA18opfuBObY3SdofuEzSfrXLs21JnkQfS2h2YTFnzpwnOntERFSa9JaGpLcDfwi8texywvbPbG8qw9cBdwEvAjaw7S6sWaUG8ODIbqfy9aFS3wDMHmeebdg+1/ZC2wuHhoYm+y1FRMQEJhUakhYB7weOsP2TVn1I0rQy/AKag9jryu6nxyQdWM6aOh64vMy2AlhchhePqh9fzqI6EHi0tRsrIiJ6YMLdU5IuAg4C9pC0HjiN5mypXYFV5czZ1eVMqd8DPizpF8AvgRNtjxxEfzfNmVhPozkGMnIc5Ezgi5JOAO4F3lzqK4HDgWHgJ8A7OvlGIyKicxOGhu3jxiifP860Xwa+PM64NcBLxqhvAg4eo27gpIn6i4iI7skV4RERUS2hERER1RIaERFRLaERERHVEhoREVEtoREREdUSGhERUS2hERER1RIaERFRLaERERHVEhoREVEtoREREdUSGhERUS2hERER1RIaERFRLaERERHVEhoREVEtoREREdUSGhERUS2hERER1RIaERFRrSo0JC2V9JCkm1u13SWtknRn+Tqj1CXpLEnDkm6U9IrWPIvL9HdKWtyq7y/ppjLPWZK0vXVERERv1G5pXAAsGlU7FbjK9nzgqvIe4DBgfnktAc6BJgCA04BXAQcAp7VC4Bzgna35Fk2wjoiI6IGq0LB9NbB5VPlIYFkZXgYc1apf6MZqYDdJewKHAqtsb7b9MLAKWFTGPcv2atsGLhy1rLHWERERPdDJMY2Ztu8vww8AM8vwXsB9renWl9r26uvHqG9vHRER0QNTciC8bCF4KpY1mXVIWiJpjaQ1Gzdu3JFtREQMtE5C48Gya4ny9aFS3wDMbk03q9S2V581Rn1769iG7XNtL7S9cGhoqINvKSIitqeT0FgBjJwBtRi4vFU/vpxFdSDwaNnFdCVwiKQZ5QD4IcCVZdxjkg4sZ00dP2pZY60jIiJ6YHrNRJIuAg4C9pC0nuYsqDOBL0o6AbgXeHOZfCVwODAM/AR4B4DtzZL+Bri2TPdh2yMH199Nc4bW04AryovtrCMiInqgKjRsHzfOqIPHmNbASeMsZymwdIz6GuAlY9Q3jbWOiIjojVwRHhER1RIaERFRLaERERHVEhoREVEtoREREdUSGhERUS2hERER1RIaERFRLaERERHVEhoREVEtoREREdUSGhERUS2hERER1RIaERFRLaERERHVEhoREVEtoREREdUSGhERUS2hERER1RIaERFRLaERERHVJh0akl4saW3r9Zik90g6XdKGVv3w1jwfkDQs6Q5Jh7bqi0ptWNKprfo8SdeU+sWSdpn8txoREZ2adGjYvsP2AtsLgP2BnwCXltGfHBlneyWApH2BY4H9gEXApyRNkzQNOBs4DNgXOK5MC/CxsqwXAg8DJ0y234iI6NxU7Z46GLjL9r3bmeZIYLntn9m+GxgGDiivYdvrbP8cWA4cKUnA64BLyvzLgKOmqN+IiJiEqQqNY4GLWu9PlnSjpKWSZpTaXsB9rWnWl9p49ecAj9h+fFT910haImmNpDUbN27s/LuJiIgxdRwa5TjDEcCXSukcYG9gAXA/8IlO1zER2+faXmh74dDQ0I5eXUTEwJo+Bcs4DLje9oMAI18BJJ0HfLW83QDMbs03q9QYp74J2E3S9LK10Z4+IiJ6YCp2Tx1Ha9eUpD1b444Gbi7DK4BjJe0qaR4wH/gucC0wv5wptQvNrq4Vtg18E3hTmX8xcPkU9BsREZPU0ZaGpGcArwfe1Sr/naQFgIF7RsbZvkXSF4FbgceBk2xvKcs5GbgSmAYstX1LWdYpwHJJHwFuAM7vpN+IiOhMR6Fh+79oDli3a2/bzvRnAGeMUV8JrByjvo7m7KqIiOgDuSI8IiKqJTQiIqJaQiMiIqolNCIiolpCIyIiqiU0IiKiWkIjIiKqJTQiIqJaQiMiIqolNCIiolpCIyIiqiU0IiKiWkIjIiKqJTQiIqJaQiMiIqolNCIiolpCIyIiqiU0IiKiWkIjIiKqJTQiIqJax6Eh6R5JN0laK2lNqe0uaZWkO8vXGaUuSWdJGpZ0o6RXtJazuEx/p6TFrfr+ZfnDZV512nNEREzOVG1pvNb2AtsLy/tTgatszweuKu8BDgPml9cS4BxoQgY4DXgVcABw2kjQlGne2Zpv0RT1HBERT9CO2j11JLCsDC8DjmrVL3RjNbCbpD2BQ4FVtjfbfhhYBSwq455le7VtAxe2lhUREV02FaFh4BuSrpO0pNRm2r6/DD8AzCzDewH3teZdX2rbq68fox4RET0wfQqW8bu2N0h6LrBK0u3tkbYtyVOwnnGVsFoCMGfOnB25qoiIgdbxlobtDeXrQ8ClNMckHiy7lihfHyqTbwBmt2afVWrbq88aoz66h3NtL7S9cGhoqNNvKSIixtFRaEh6hqTfHBkGDgFuBlYAI2dALQYuL8MrgOPLWVQHAo+W3VhXAodImlEOgB8CXFnGPSbpwHLW1PGtZUVERJd1untqJnBpOQt2OvDPtr8u6Vrgi5JOAO4F3lymXwkcDgwDPwHeAWB7s6S/Aa4t033Y9uYy/G7gAuBpwBXlFRERPdBRaNheB7xsjPom4OAx6gZOGmdZS4GlY9TXAC/ppM+IiJgauSI8IiKqJTQiIqJaQiMiIqolNCIiolpCIyIiqiU0IiKiWkIjIiKqJTQiIqJaQiMiIqolNCIiolpCIyIiqiU0IiKiWkIjIiKqJTQiIqJaQiMiIqolNCIiolpCIyIiqiU0IiKiWkIjIiKqJTQiIqJaQiMiIqpNOjQkzZb0TUm3SrpF0l+W+umSNkhaW16Ht+b5gKRhSXdIOrRVX1Rqw5JObdXnSbqm1C+WtMtk+42IiM51sqXxOPBXtvcFDgROkrRvGfdJ2wvKayVAGXcssB+wCPiUpGmSpgFnA4cB+wLHtZbzsbKsFwIPAyd00G9ERHRo0qFh+37b15fhHwG3AXttZ5YjgeW2f2b7bmAYOKC8hm2vs/1zYDlwpCQBrwMuKfMvA46abL8REdG5KTmmIWku8HLgmlI6WdKNkpZKmlFqewH3tWZbX2rj1Z8DPGL78VH1iIjokY5DQ9IzgS8D77H9GHAOsDewALgf+ESn66joYYmkNZLWbNy4cUevLiJiYHUUGpKeShMYX7D9FQDbD9reYvuXwHk0u58ANgCzW7PPKrXx6puA3SRNH1X/NbbPtb3Q9sKhoaFOvqWIiNiOTs6eEnA+cJvtv2/V92xNdjRwcxleARwraVdJ84D5wHeBa4H55UypXWgOlq+wbeCbwJvK/IuByyfbb0REdG76xJOM63eAtwE3SVpban9Nc/bTAsDAPcC7AGzfIumLwK00Z16dZHsLgKSTgSuBacBS27eU5Z0CLJf0EeAGmpCKiIgemXRo2P43QGOMWrmdec4AzhijvnKs+WyvY+vurYiI6LFcER4REdUSGhERUS2hERER1RIaERFRLaERERHVEhoREVEtoREREdUSGhERUS2hERER1RIaERFRLaERERHVEhoREVEtoREREdUSGhERUS2hERER1RIaERFRLaERERHVEhoREVEtoREREdUm/Yzw2PnNPfVrHS/jnjPfMAWdRMTOIlsaERFRre9DQ9IiSXdIGpZ0aq/7iYgYZH29e0rSNOBs4PXAeuBaSSts39rbziJiR8vu0/7U71saBwDDttfZ/jmwHDiyxz1FRAysfg+NvYD7Wu/Xl1pERPRAX++eqiVpCbCkvP2xpDs6XOQewA87XEan+qEHmKAPfaz3PXRRP/QxYQ8D9DPJZzG1PfxWzUT9HhobgNmt97NKbRu2zwXOnaqVSlpje+FULW9n7aFf+uiHHvqlj37ooV/66Ice+qWPbvbQ77unrgXmS5onaRfgWGBFj3uKiBhYfb2lYftxSScDVwLTgKW2b+lxWxERA6uvQwPA9kpgZZdXO2W7ujrQDz1Af/TRDz1Af/TRDz1Af/TRDz1Af/TRtR5ku1vrioiInVy/H9OIiIg+ktCIiIhqCY2IiKjW9wfCIyIAJInm1kIjd4XYAHzXOTDbVQN/IFzSocBRbPsP8XLbXx+0Pvqkh+nACcDRwPPbfQDn2/5Fl/rIZ7G1j374LA4BPgXcydYLfGcBLwTebfsbXeyl5+HVyx4GOjQk/QPwIuBCmvtaQfMP8XjgTtt/OSh99EMPpY+LgEeAZaP6WAzsbvstXeghn8XWHvrls7gNOMz2PaPq84CVtvfpUh89D69e9zDoofF92y8aoy7g+7bnD0of/dDD9vqYaFw3eshnsU2925/FncA+th8fVd8FuNX2C7vUR8/Dq9c9DPqB8J9KeuUY9VcCPx2wPvqhB4DNko6R9Kt/m5KeIuktwMNd6iGfxVb98lkspXmezimS/ri8TgGuAc7vYh/T2brF1bYBeOog9DDoB8LfDpwj6TfZ+kOYDTxaxg1SH/3QAzT3F/sY8ClJI78YdwO+WcZ1w9vJZzHi7fTBZ2H7byVdRvM8nVeX8gbgrV1+KNtIeC1n62MbZtP8PLoVXj3tYaB3T42Q9DxaB5RsPzCoffRDD61engNge1OP1p/PYuv6++az6DVJ+9CEV/sgdFefKNrLHhIasdOQ9LxB/mXVls9iK0mn2z69130MikE/pjEuSdf3ugfojz76oYeim/uux5TPYqs++iyu63UD0ITXIPSQLY2IiCkg6X/a/pcnew8JjT4iaSbb7jd+sMf9HGG75w+9krS77c09WG9f/DwkDdGch78FWGf7xz3ooeefhaTXAn9Ec9B3C/B94DO2h7vdyyAb6LOnJM0GPk7zn+EK4OMjV9lKusz2UV3qYwHwaeDZtC7WkfQIzcU6O3w3gKQ3ji4BZ5erkrH9lR3dQ+njQ7Y/Uob3BS4DnlquC3iL7Wu60EPPfx6lj32Bs4C5wBzgBuC5kr4N/KXtR7vQQ798Fn8LPA+4qny9G7gL+JKkj9r+Ujf6GKe3rlwz01rfC4APAf8JnAl8kuaMstuA942+fmPK2R7YF7AKOBFYAPwj8B3gOWXcDV3sYy3wqjHqBwLf61IPvwC+SnM632fL60fl69IufhbXt4a/RnMREzS3TPjOoPw8yvpWAy9uff/LyvA7gUsG7LO4qTU8Hfj3MjwDuLmLffwIeKy8flReW0bqXerhauDPgFOBm4G/otn6OgH41x2+/m592P34AtaOev8nwC3A3u1fXl3o487tjBvuUg+vpPkr7s9atbt78DNph8YNo8Z1Jcj74edR1vW9Ue/bn81tg/ZZ0Nw6BZqtrtWtcbd0sY+zaG6pMrNVu7tb6y/ru6E1/IPxxu2o10DvnqLZ7fEbtn8KYPvzkh6geSb5M7rYxxWSvkbzj7F9sc7xQFduCmf7WkmvB/5c0jeBU4BeHPB6gaQVNLvHZkl6uu2flHHduuK25z+P4i5J/xv4V+CNNH/1I+mpdO/Mx375LD4K3CDp+8CLaf7SHjne871uNWH7LyTtD1xULjb8J7r//+SXkl5Es8vw6ZIW2l4j6YXAtB298oE+EC7pvTR/vX17VP3lwN/Zfn0XezmMsS/W6fbz0ZH0fOAfgIW2X9Dldf/+qNJ1tn9cDsS+yfbZXeqj5z8PSbsBfw3sS/OL8UzbP5L0bJr7MK3uUh89/yxKH7sDL6DZwnmkm+seo5enACcDxwB7237+BLNM5boPprlh4S9pdlW+F3gZ8CzgnbYv36HrH+TQiIidi6SFtM6esn17j/vZE3h5L/64G9XHHsDDtrfs6HUN9MV9kqZLepekr0u6sbyukHRi2QXQc5LOTQ+NbvUh6aWt4adK+pCkFZI+Kunp3eihX/qQdHL5hYSkvSVdLelhSddI+u1u9FDW/fuS1tCcLbQUWAKcL+lb5SzIbvWxi6TjJf1BKb0WOFzSSd36ndHq4eDy/o+B04F3daOHgd7SUB88r6D0sft4o2gOhs4ahB76pQ9J19t+RRn+BPAcmrPIjqI5u+74Hd1Dv/Qh6Rbb+5Xhr9FcF3GppIOAM2z/zo7uoaz7BuAQ2xvV3AL8720fXY7Dvc/2IV3q4ws0Z289neZ3xzOBrwAH0/w+Xfxk72HQD4Tv718/v3o9sLoccOuWjcC9NL8YR7i8f+4A9dAvfbTXfTDwStu/kHQ1XTzo2id9tH9HPNf2pQC2v6XmzrfdMs32xjL8A+C3Sh+r1Dwoqlt+2/ZLy/VLG4Dn294i6fN072fS0x4GPTQ2SzoG+LLtX8KvDnAdQ/eeVwCwDjjY9g9Gj5B03xjTP1l76Jc+ni3paJrdt7u6XPBp25K6uWneD31cIukC4MPApZLeA1wKvI7ml3e3rJF0Ps2ZZEcA3wIou+l2+BlDLU9R8+CnZ9D8pf9sYDOwK907u6+nPQx6aIx+XoFofgDdfF4BNGcqzWDs/4R/N0A99EsfV9P8YoJmq3Om7QfV3B78h13qoS/6sP1BSe8ALqK5fmlXmuMJlwFv7UYPxbtozhR6NfB/aY5rQLMVemgX+zgfuJ0mqD5Ic0X6OpqLHZcPQg8DfUyjTeV5BcD/sf0nPW0GkHRht/ad93MP/dJHP/TQL31I+pztt/Wyh14qp6Rj+z/LadF/QHOR3XcHoYeB3tJQcxHZaK8bqds+Yozx3ehDwGvLP4au9NEPPfRLH9v5d9Hrz6LrffTR/5FnAu+nuchxNvBzmntPfdr2Bd3oYUT5RT2k5nquLcDX3eWbSPayh4EODZozpW4FPsPWg62vBD7R5T5m09y+pN3Hwi730Q899EsfY/XQL/8uut1Hv/wf+QLNsZRFwJtp9ucvBz4k6UW2/7obTWjsm0gOlZMTunUTyd72sKPvU9LPL5oDjO+luXHhglJbN4h99EMP/dJHP/TQL330Qw9lnaPvw3Vtq7/bu9hHP9xEsqc95JgGIGkWze2FHwSOsD1nUPvohx76pY9+6KFf+uh1D5K+A7zf9r9JOpLmtuyHlnF32H5xl/r4nu2Xtd63r6W5zfY+T/YeBn33FAC21wPHSHoDzS2PB7aPfuihX/rohx76pY8+6OHPgPMkzafZZfen8KsbFnblfmRFP9xEsqc9ZEsjInYKkvZm64Hwx2me3PfPtrsWYuqDm0j2uoeERkT0PUl/AfwhzbUrh9Mc/H0EOJpmV9W3etjbc20/1Kv1d7uHhEZE9D1JN9EciN9SrgJfafsgSXOAy22/vEt9jL43moDrgJfT/D7d4c+y73UPOaYRETuL6TTXJOxKc5M+bP9A3b0j9Q9p7o3WthdwPc0pyd14/kxPexjoW6NHxE7jM8C1ks4D/oNy8LscCN/hf923vA+4g+YMsnm25wHry3C3HljW0x6yeyoidgqS9gP2AW52Dx++1Dr9+D7gNJprSLr9hMue9ZDQiIiYBElH0JzFNNf28walh4RGRMQkSXoazTPCb5b0DtuffbL3kNCIiJgCkn7Qq7sGdLOHnD0VEVFJ0o3jjQJmDkIPCY2IiHozaR76NPrJngK+Mwg9JDQiIup9FXim7bWjR0j61iD0kGMaERFRLRf3RUREtYRGRERUS2hERES1hEbEFJM0V9LNk5z3IEn/Y4JpTpR0/FSuN6JWzp6KqCRpmu0tO3g1BwE/ZjunTtr+9A7uIWJc2dKI4Fd/pd8u6QuSbpN0iaSnS7pH0sckXU/zuNMFklZLulHSpZJmlPn3l/Q9Sd8DTmot9+2S/qn1/quSDirDiyRdX+a7StJc4ETgvZLWSnrNOL2eLul/bW+9ETtKQiNiqxcDn7K9D81zsN9d6ptsv8L2cuBC4BTbLwVuornDKMBngT+3/bKaFZVbep8H/FGZ5xjb9wCfBj5pe4Ht/1exqCe03ohOJTQitrrP9r+X4c8Dv1uGLwYoz2Dezfa3S30Z8Hvlmc272b661D9Xsa4Dgatt3w0wmaetTXK9ER1JaERsNfpK15H3/9XBMh9n2/9nv9HBsiJ6LqERsdUcSa8uw38M/Ft7pO1HgYdbxxreBnzb9iPAI5JGtkze2prtHmCBpKdImg0cUOqrabZS5sE2z33+EfCbNc1OsN6IHSKhEbHVHcBJkm4DZgDnjDHNYuDj5U6jC4APl/o7gLMlraW5cdyIfwfuBm4FzqJ5jjO2NwJLgK+Ug9gXl+n/BTh6ewfCRxlvvRE7RO49FUFz9hTwVdsv6XErEX0tWxoREVEtWxoRfUrSB4FjRpW/ZPuMXvQTAQmNiIh4ArJ7KiIiqiU0IiKiWkIjIiKqJTQiIqJaQiMiIqr9f3jPPp5eiR11AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the ditribution of 'product_id'\n",
    "plot_count_per_id(milhojas_frambuesa_transacciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets explore the names distribution of 414, 459 and 9999:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "milhojas frambuesa 2                       167542\n",
       "milhojas frambuesa 1                         1311\n",
       "tiras de milhojas rellena solo de crema       532\n",
       "milhojas frambuesa 3                          437\n",
       "milhojas frambuesa 2 solo crema               399\n",
       "Name: desc_normalized, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Names distribution for product_id = 459:\n",
    "df_with_normalized_descriptions_transactions[df_with_normalized_descriptions_transactions['product_id']==459.0]['desc_normalized'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tarta muy tres chocolates 2    4788\n",
       "tarta milhojas frambuesa 2     4123\n",
       "tarta limon 3                  3420\n",
       "tarta milhojas 2               1824\n",
       "tarta tres chocolates 2        1805\n",
       "Name: desc_normalized, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Names distribution for product_id = 414:\n",
    "df_with_normalized_descriptions_transactions[df_with_normalized_descriptions_transactions['product_id']==414.0]['desc_normalized'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Names distribution for product_id = 9999:\n",
    "df_with_normalized_descriptions_transactions[(df_with_normalized_descriptions_transactions['product_id']==9999.0) & (df_with_normalized_descriptions_transactions['desc_normalized'].str.contains('milhoja'))]['desc_normalized'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems there is a strong correlation with id 459, however 459 it also includes other types of 'milhojas'. In addition, 'milhojas de frambuesa 2º' is also included in id number 414, which seems to be a multiple 'tartas' id, and id 9999, which is the id used for custom orders.\n",
    "\n",
    "For these reasons, we decide to filter milhojas based on the description (filtering the dataframe with only disting 'product_id' and 'des_normalized' values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save unique product_id and product_description:\n",
    "milhojas_frambuesa = filter_milhojas(unique_normalized_decriptions)\n",
    "milhojas_frambuesa['target_names_prod_by_prod'] = 'milhojas frambuesa'\n",
    "list_of_dfs.append(milhojas_frambuesa)\n",
    "milhojas_frambuesa.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Matching: croissant petite\n",
    "\n",
    "From the analysis performed in notebook \"x01-transactions_to_partial_results-yy.ipynb\", it was concluded that 'croissant petite' had an strong correlation with id number '103' however, before commiting to filtering by that ID, lets plot the distribution of count of lines per ID, that satisfies the filters of the following function, from the transaction dataset, in order to :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_croissant_petit (df):\n",
    "    croissant = df[df['desc_normalized'].str.contains('croissant')].copy()\n",
    "    croissant_petite = croissant[croissant['desc_normalized'].str.contains('petit')].copy()\n",
    "    return croissant_petite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results:\n",
    "transactions_croissant_petite = filter_croissant_petit(df_with_normalized_descriptions_transactions)\n",
    "transactions_croissant_petite.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count_per_id(transactions_croissant_petite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that id '103' represents the 'croissant petite';  in fact, after reviweing the data with the client, he suggested only taking 103."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving id=103 as croissant petit\n",
    "croissant_petit = unique_normalized_decriptions[unique_normalized_decriptions['product_id']==103.0].copy()\n",
    "croissant_petit['target_names_prod_by_prod'] = 'croissant petit'\n",
    "list_of_dfs.append(croissant_petit)\n",
    "croissant_petit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Matching: croissant\n",
    "\n",
    "From the analysis performed in notebook \"x01-transactions_to_partial_results-yy.ipynb\", it was concluded that 'croissant simple' had an strong correlation with id number '100' and '101' however, before commiting to filtering by that ID, lets plot the distribution of count of lines per ID, that satisfies the filters of the following function, from the transaction dataset, in order to :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_croissant_simple (df):\n",
    "    croissant = df[df['desc_normalized'].str.contains('croissant')].copy()\n",
    "    croissant_simple = croissant[~croissant['desc_normalized'].str.contains('petit|tira|masa')].copy()\n",
    "    return croissant_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_croissant_simple = filter_croissant_simple(df_with_normalized_descriptions_transactions)\n",
    "transactions_croissant_simple.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count_per_id(transactions_croissant_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, the correlation seems to exsit for several ids... Lets explore a bit more... Lets plot the most common description from each id and see if we find any pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {}\n",
    "#for i in transactions_croissant_simple_by_id.index:\n",
    "for i in transactions_croissant_simple_by_id.index:\n",
    "    aux = df_with_normalized_descriptions_transactions[df_with_normalized_descriptions_transactions['product_id']==i][['desc_normalized','product_id']].groupby('desc_normalized').count().reset_index()\n",
    "    name = aux.sort_values(by = 'product_id', ascending = False)['desc_normalized'].iloc[0]\n",
    "    names[i]=name\n",
    "    \n",
    "names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that they have different types of croissant. Checking with the client, he suggested taking only: 100.0 and 101.0 that contains the word 'croissant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "croissant_simple = unique_normalized_decriptions[(unique_normalized_decriptions['product_id']==100.0)|(unique_normalized_decriptions['product_id']==101.0)].copy()\n",
    "croissant_simple = croissant_simple[croissant_simple['desc_normalized'].str.contains('croissant')]\n",
    "croissant_simple = croissant_simple[~croissant_simple['desc_normalized'].str.contains('petit|tira|masa')].copy()\n",
    "\n",
    "croissant_simple['target_names_prod_by_prod'] = 'croissant simple'\n",
    "list_of_dfs.append(croissant_simple)\n",
    "croissant_simple.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4 Matching: tarta mousse tres chocolates\n",
    "\n",
    "From the analysis performed in notebook \"x01-transactions_to_partial_results-yy.ipynb\", it was concluded that 'mousse tres chocolates' had no correlation with a particular product_id, therefore the analysis will be based on the description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_mousse_tres_chocolates (df):\n",
    "    tarta = df[df['desc_normalized'].str.contains('tarta')].copy()\n",
    "    mousse = tarta[tarta['desc_normalized'].str.contains('mousse|mus')].copy()\n",
    "    mousse_tres = mousse[mousse['desc_normalized'].str.contains('tres|3')].copy()\n",
    "    mousse_tres = mousse_tres[~mousse_tres['desc_normalized'].str.contains('mini')].copy()\n",
    "    mousse_tres_chocolates = mousse_tres[mousse_tres['desc_normalized'].str.contains('chocolate')].copy()\n",
    "    return mousse_tres_chocolates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_mousse_tres_chocolates = filter_mousse_tres_chocolates(df_with_normalized_descriptions_transactions)\n",
    "transactions_mousse_tres_chocolates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count_per_id(transactions_mousse_tres_chocolates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting... Lets explore it more by having a look at the full distribution of names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in transactions_mousse_tres_chocolates['product_id'].unique():\n",
    "    to_print = df_with_normalized_descriptions_transactions[df_with_normalized_descriptions_transactions['product_id']==i]['desc_normalized'].value_counts().head()\n",
    "    print(\"***Plotting id: {} \".format(i))\n",
    "    print(\"-\")\n",
    "\n",
    "    print(to_print)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that 'tarta mousse tres chocolates' is all over the place... So the easiest is to filter by product description\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mousse_tres_chocolates = filter_mousse_tres_chocolates(unique_normalized_decriptions)\n",
    "mousse_tres_chocolates['target_names_prod_by_prod'] = 'mousse tres chocolates'\n",
    "list_of_dfs.append(mousse_tres_chocolates)\n",
    "mousse_tres_chocolates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.5 Matching: tarta de manzana 2\n",
    "From the analysis performed in notebook \"x01-transactions_to_partial_results-yy.ipynb\", it was concluded that 'tarta de manzana 2' had no correlation with a particular product_id, therefore the analysis will be based on the description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tarta_manzana_2 (df):\n",
    "    manzana_tarta = df[df['desc_normalized'].str.contains('manzana')]\n",
    "    #manzana_tarta = manzana[manzana['desc_normalized'].str.contains('tarta')].copy() #Removed because we saw it had better fit\n",
    "    manzana_tarta = manzana_tarta[~manzana_tarta['desc_normalized'].str.contains('caramelo')].copy()\n",
    "    manzana_tarta_dos = manzana_tarta[manzana_tarta['desc_normalized'].str.contains('dos|2')].copy()\n",
    "\n",
    "    return manzana_tarta_dos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_manzana_tarta_dos=filter_tarta_manzana_2(df_with_normalized_descriptions_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count_per_id(transactions_manzana_tarta_dos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we found a winner!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manzana_tarta_dos = filter_tarta_manzana_2(unique_normalized_decriptions)\n",
    "manzana_tarta_dos['target_names_prod_by_prod'] = 'tarta de manzana'\n",
    "list_of_dfs.append(manzana_tarta_dos)\n",
    "manzana_tarta_dos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.6 Matching: palmera de chocolate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_palmera_chocolate (df):\n",
    "    palmera = df[df['desc_normalized'].str.contains('palmera')]\n",
    "    palmera_chocolate = palmera[palmera['desc_normalized'].str.contains('chocolate|trufa')].copy() #Added trufa after reviwing results\n",
    "    return palmera_chocolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_palmera_chocolate = filter_palmera_chocolate(df_with_normalized_descriptions_transactions)\n",
    "transactions_palmera_chocolate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count_per_id(transactions_palmera_chocolate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we have a winner!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palmera_chocolate = filter_palmera_chocolate(unique_normalized_decriptions)\n",
    "palmera_chocolate['target_names_prod_by_prod'] = 'palmera chocolate'\n",
    "\n",
    "list_of_dfs.append(palmera_chocolate)\n",
    "palmera_chocolate.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.7 Matching: tarta ópera "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tarta_opera(df):\n",
    "    opera = df[df['desc_normalized'].str.contains('opera')]\n",
    "    opera_tarta = opera[opera['desc_normalized'].str.contains('tarta')].copy()\n",
    "    return opera_tarta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_tarta_opera = filter_tarta_opera(df_with_normalized_descriptions_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count_per_id(transactions_tarta_opera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in transactions_tarta_opera['product_id'].unique():\n",
    "    to_print = df_with_normalized_descriptions_transactions[df_with_normalized_descriptions_transactions['product_id']==i]['desc_normalized'].value_counts().head()\n",
    "    print(\"***Plotting id: {} \".format(i))\n",
    "    print(\"-\")\n",
    "\n",
    "    print(to_print)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, all over the place, so we decided to use the description filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tarta_opera = filter_tarta_opera(unique_normalized_decriptions)\n",
    "tarta_opera['target_names_prod_by_prod'] = 'tarta opera'\n",
    "\n",
    "list_of_dfs.append(tarta_opera)\n",
    "tarta_opera.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.9 Matching: postre de fresas y mascarpone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_postre_fresas_mascarpone (df):\n",
    "    postre = df[df['desc_normalized'].str.contains('postre')]\n",
    "    postre_fresa = postre[postre['desc_normalized'].str.contains('fresa')].copy()\n",
    "    postre_fresa = postre_fresa[~postre['desc_normalized'].str.contains('eclair')].copy() #Client indication\n",
    "    postre_fresa = postre_fresa[~postre['desc_normalized'].str.contains('tartaleta')].copy() #Client indication\n",
    "\n",
    "    postre_fresa_mascarpone = postre_fresa[postre_fresa['desc_normalized'].str.contains('mascarpone')].copy()\n",
    "    return postre_fresa_mascarpone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_postre_fresas_mascarpone = filter_postre_fresas_mascarpone(df_with_normalized_descriptions_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count_per_id(transactions_postre_fresas_mascarpone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in transactions_postre_fresas_mascarpone['product_id'].unique():\n",
    "    to_print = df_with_normalized_descriptions_transactions[df_with_normalized_descriptions_transactions['product_id']==i]['desc_normalized'].value_counts().head()\n",
    "    print(\"***Plotting id: {} \".format(i))\n",
    "    print(\"-\")\n",
    "\n",
    "    print(to_print)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems that the filter is working :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postre_fresas_mascarpone = filter_postre_fresas_mascarpone(unique_normalized_decriptions)\n",
    "\n",
    "postre_fresas_mascarpone['target_names_prod_by_prod'] = 'postre de fresas y mascarpone'\n",
    "list_of_dfs.append(postre_fresas_mascarpone)\n",
    "postre_fresas_mascarpone.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.9 Matching: tortel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tortel (df):\n",
    "    tortel = df[df['desc_normalized'].str.contains('tortel')].copy()\n",
    "    tortel = tortel[~tortel['desc_normalized'].str.contains('tortellini|mini')].copy()\n",
    "\n",
    "    return tortel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_tortel = filter_tortel(df_with_normalized_descriptions_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count_per_id(transactions_tortel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in transactions_tortel['product_id'].unique():\n",
    "    to_print = df_with_normalized_descriptions_transactions[df_with_normalized_descriptions_transactions['product_id']==i]['desc_normalized'].value_counts().head()\n",
    "    print(\"***Plotting id: {} \".format(i))\n",
    "    print(\"-\")\n",
    "\n",
    "    print(to_print)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tortel = filter_tortel(unique_normalized_decriptions)\n",
    "\n",
    "tortel['target_names_prod_by_prod'] = 'tortel'\n",
    "list_of_dfs.append(tortel)\n",
    "tortel.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.10 Matching: baguette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_baguette (df):\n",
    "    baguette = df[df['desc_normalized'].str.contains('baguette|baguete|baguet')].copy()\n",
    "    return baguette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_baguette = filter_baguette(df_with_normalized_descriptions_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count_per_id(transaction_baguette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in transaction_baguette['product_id'].unique():\n",
    "    to_print = df_with_normalized_descriptions_transactions[df_with_normalized_descriptions_transactions['product_id']==i]['desc_normalized'].value_counts().head()\n",
    "    print(\"***Plotting id: {} \".format(i))\n",
    "    print(\"-\")\n",
    "\n",
    "    print(to_print)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per client indications we take id = 115.0 containing baguette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baguette = unique_normalized_decriptions[unique_normalized_decriptions['product_id']==115.0].copy()\n",
    "baguette = baguette[baguette['desc_normalized'].str.contains('baguette|baguete|baguet')].copy()\n",
    "\n",
    "baguette['target_names_prod_by_prod'] = 'baguette'\n",
    "list_of_dfs.append(baguette)\n",
    "baguette.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets now concatenate results, merge them back to the  full list of normalized descriptions and evaluate its effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets concatenate the results:\n",
    "list_of_products_df = pd.concat(list_of_dfs, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_products_df[list_of_products_df['product_id']==414.0]['desc_normalized'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_products_df[list_of_products_df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_desc_normalezed_vs_prod_by_prod = pd.merge(df_with_normalized_descriptions_transactions, list_of_products_df[['desc_normalized','target_names_prod_by_prod']],how='left',on = 'desc_normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK - 'df_with_normalized_descriptions' has the same size as 'df_with_normalized_descriptions' \n"
     ]
    }
   ],
   "source": [
    "#Control merge size:\n",
    "if (df_with_normalized_descriptions_transactions.shape[0] == df_desc_normalezed_vs_prod_by_prod.shape[0] ): \n",
    "    test1 = \"OK - 'df_with_normalized_descriptions_transactions' has the same size as 'df_with_normalized_descriptions_transactions' \"\n",
    "else:\n",
    "    test1 = \"ERROR - 'df' has NOT the same size as 'df_with_normalized_descriptions_transactions' \"\n",
    "\n",
    "print(test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE:*\n",
    "\n",
    "During the first executions of code with the full transactions file, this error was failing; 'df' had less rows than 'df_with_normalized_descriptions_transactions'. The reason for this was not easy to identify, however digging we found that that in the normalized description two products descriptions ware naming two different products in the description, however this was not the case for the raw description (before the spell-cheacker):\n",
    "\n",
    "for example:\n",
    "- Normalized prod description: 'tarta mousse 3 chocolates de 20 raciones con escrito sobre la tarta manzana y mini felicidades'\n",
    "- Raw description: 'TARTA MOUSSE 3 CHOCOLATES DE 20 RACIONES CON ESCRITO SOBRE LA TARTA:  MARIANA Y DANI FELICIDADES'\n",
    "\n",
    "Basically, the spell-corrector was solving some problems; normalizing 'trata' , 'taaarta' under 'tarta', but adding a new one: normalizing words that it doesnt know, that may be a correct word, to a word that it knows: 'MARIANA' to 'manzana'... Ofcourse this is a weakness, however from the manual inspections that were performed, it doesnt seem to happen often.\n",
    "\n",
    "How we solve it by adding to the bakery products dataset: \n",
    "- A list of the most common male and female spanish names: in order to avoid confusion in the names\n",
    "\n",
    "sources of the datasets:\n",
    "- spanish names:https://www.ine.es/dyngs/INEbase/es/operacion.htm?c=Estadistica_C&cid=1254736177009&menu=resultados&secc=1254736195454&idp=1254734710990\n",
    "\n",
    "\n",
    "Also, in this case we added some names that we found to the excel; the right thing to do should we had more time, would be to polish the dataset, by adding not just mallorca catalogue and names, but also a book in spanish. Perhaps, it would also be interesting to applying NLP to identify NAMES from the product descriptions and add them to the products dataset..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now check how effective it was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desc_normalized</th>\n",
       "      <th>target_names_prod_by_prod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pechuga de rellena de higos y setas</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>surtido novedad 1 4 g</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>surtido novedad de 1 2 g</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rollito de salmon del de mouselina de marcos</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>todos de alcachofas rellenos de mario</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>magret de pato con citricos</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>albondigas de carne</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cordero al cherry</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ensalada de pancitos con gambas</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>verduras salteadas con jamon</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                desc_normalized target_names_prod_by_prod\n",
       "0           pechuga de rellena de higos y setas                       NaN\n",
       "1                         surtido novedad 1 4 g                       NaN\n",
       "2                      surtido novedad de 1 2 g                       NaN\n",
       "3  rollito de salmon del de mouselina de marcos                       NaN\n",
       "4         todos de alcachofas rellenos de mario                       NaN\n",
       "5                   magret de pato con citricos                       NaN\n",
       "6                           albondigas de carne                       NaN\n",
       "7                             cordero al cherry                       NaN\n",
       "8               ensalada de pancitos con gambas                       NaN\n",
       "9                  verduras salteadas con jamon                       NaN"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets look at the first 10 description names\n",
    "df_desc_normalezed_vs_prod_by_prod[['desc_normalized','target_names_prod_by_prod']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desc_normalized</th>\n",
       "      <th>target_names_prod_by_prod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>mousse chocolate blanco</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>postres mousse chocolate en vasito</td>\n",
       "      <td>mousse tres chocolates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>postres mousse frutas bosque</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>postres mousse frutas bosque</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>postres mousse chocolate en vasito</td>\n",
       "      <td>mousse tres chocolates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>postres mousse de limon</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>petit yogur mousse praline</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>postres mousse frutas bosque</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>postres mousse chocolate en vasito</td>\n",
       "      <td>mousse tres chocolates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>mousse 3 chocolates 3</td>\n",
       "      <td>mousse tres chocolates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133</th>\n",
       "      <td>mousse 3 chocolates 1</td>\n",
       "      <td>mousse tres chocolates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>mousse 3 chocolates 2</td>\n",
       "      <td>mousse tres chocolates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>postres mousse tres chocolates</td>\n",
       "      <td>mousse tres chocolates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>petit yogur mousse praline</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>postres mousse de limon</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         desc_normalized target_names_prod_by_prod\n",
       "385              mousse chocolate blanco                       NaN\n",
       "519   postres mousse chocolate en vasito    mousse tres chocolates\n",
       "521         postres mousse frutas bosque                       NaN\n",
       "590         postres mousse frutas bosque                       NaN\n",
       "592   postres mousse chocolate en vasito    mousse tres chocolates\n",
       "658              postres mousse de limon                       NaN\n",
       "677           petit yogur mousse praline                       NaN\n",
       "1074        postres mousse frutas bosque                       NaN\n",
       "1076  postres mousse chocolate en vasito    mousse tres chocolates\n",
       "1132               mousse 3 chocolates 3    mousse tres chocolates\n",
       "1133               mousse 3 chocolates 1    mousse tres chocolates\n",
       "1134               mousse 3 chocolates 2    mousse tres chocolates\n",
       "1135      postres mousse tres chocolates    mousse tres chocolates\n",
       "1162          petit yogur mousse praline                       NaN\n",
       "1166             postres mousse de limon                       NaN"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets now review the effectiveness filtering by 'mousse '. The expected result is that all 'mousse 3 chocolates' match\n",
    "df_desc_normalezed_vs_prod_by_prod.loc[df_desc_normalezed_vs_prod_by_prod['desc_normalized'].str.contains('mousse'),['desc_normalized','target_names_prod_by_prod'] ].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better! lets now check that the data integrity has not been compromised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Test that data has not been corrputed\n",
    "\n",
    "To test the integrity of the data, the original dataset should be the same as the last dataset without that we added, in other words, without the columns with the normalized descriptuons, and the target names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (30550252, 6)\n",
      "Resulting dataset shape: (30550252, 8)\n"
     ]
    }
   ],
   "source": [
    "# First, lets check the size of both dataframes:\n",
    "print(\"Original dataset shape: {}\".format(df.shape))\n",
    "print(\"Resulting dataset shape: {}\".format(df_desc_normalezed_vs_prod_by_prod.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape looks good, we were expecting the resulting dataset to have to columns more. Lets now evauate if they are actually the same dataset if we remove the added columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting original columnsd from the resulting df\n",
    "df_result = df_desc_normalezed_vs_prod_by_prod.loc[:, df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lets compare it with the original dataset, sorting them out in the same way:\n",
    "df_result_sorted = df_result.sort_values(by = ['order_date','store','description','product_id', 'units_ordered']).reset_index().drop('index', axis = 1)\n",
    "df_original_sorted = df.sort_values(by = ['order_date','store','description',  'product_id', 'units_ordered']).reset_index().drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>description</th>\n",
       "      <th>order_date</th>\n",
       "      <th>section</th>\n",
       "      <th>store</th>\n",
       "      <th>units_ordered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>615.0</td>\n",
       "      <td>ALASKAS FRAMBUESA</td>\n",
       "      <td>1/1/2008 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>AaUP</td>\n",
       "      <td>2,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>172.0</td>\n",
       "      <td>1/2 PAN DE MOLDE ENVUELTO</td>\n",
       "      <td>1/1/2008 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>AaUP</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9999.0</td>\n",
       "      <td>3 RACIONES DE ENSALADA DE CAPON</td>\n",
       "      <td>1/1/2008 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>AaUP</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9999.0</td>\n",
       "      <td>3 RACIONES VERDURAS SALTEADAS</td>\n",
       "      <td>1/1/2008 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>AaUP</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3252.0</td>\n",
       "      <td>ACETATO 11 POOL                          NUEVO</td>\n",
       "      <td>1/1/2008 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>AaUP</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id                                     description  \\\n",
       "0       615.0                               ALASKAS FRAMBUESA   \n",
       "1       172.0                       1/2 PAN DE MOLDE ENVUELTO   \n",
       "2      9999.0                 3 RACIONES DE ENSALADA DE CAPON   \n",
       "3      9999.0                   3 RACIONES VERDURAS SALTEADAS   \n",
       "4      3252.0  ACETATO 11 POOL                          NUEVO   \n",
       "\n",
       "         order_date  section store units_ordered  \n",
       "0  1/1/2008 0:00:00        0  AaUP          2,00  \n",
       "1  1/1/2008 0:00:00        0  AaUP          0,00  \n",
       "2  1/1/2008 0:00:00        0  AaUP          0,00  \n",
       "3  1/1/2008 0:00:00        0  AaUP          0,00  \n",
       "4  1/1/2008 0:00:00        0  AaUP          0,00  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>description</th>\n",
       "      <th>order_date</th>\n",
       "      <th>section</th>\n",
       "      <th>store</th>\n",
       "      <th>units_ordered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>615.0</td>\n",
       "      <td>ALASKAS FRAMBUESA</td>\n",
       "      <td>1/1/2008 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>AaUP</td>\n",
       "      <td>2,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>172.0</td>\n",
       "      <td>1/2 PAN DE MOLDE ENVUELTO</td>\n",
       "      <td>1/1/2008 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>AaUP</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9999.0</td>\n",
       "      <td>3 RACIONES DE ENSALADA DE CAPON</td>\n",
       "      <td>1/1/2008 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>AaUP</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9999.0</td>\n",
       "      <td>3 RACIONES VERDURAS SALTEADAS</td>\n",
       "      <td>1/1/2008 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>AaUP</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3252.0</td>\n",
       "      <td>ACETATO 11 POOL                          NUEVO</td>\n",
       "      <td>1/1/2008 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>AaUP</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id                                     description  \\\n",
       "0       615.0                               ALASKAS FRAMBUESA   \n",
       "1       172.0                       1/2 PAN DE MOLDE ENVUELTO   \n",
       "2      9999.0                 3 RACIONES DE ENSALADA DE CAPON   \n",
       "3      9999.0                   3 RACIONES VERDURAS SALTEADAS   \n",
       "4      3252.0  ACETATO 11 POOL                          NUEVO   \n",
       "\n",
       "         order_date  section store units_ordered  \n",
       "0  1/1/2008 0:00:00        0  AaUP          2,00  \n",
       "1  1/1/2008 0:00:00        0  AaUP          0,00  \n",
       "2  1/1/2008 0:00:00        0  AaUP          0,00  \n",
       "3  1/1/2008 0:00:00        0  AaUP          0,00  \n",
       "4  1/1/2008 0:00:00        0  AaUP          0,00  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_original_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK - The original dataset is the similar to the resulting dataset\n"
     ]
    }
   ],
   "source": [
    "# Now that they have the same columns, and are sorted using the same criteria, lets evaluate if they are the same:\n",
    "comparison_result = df_result_sorted.equals(df_original_sorted)\n",
    "\n",
    "if comparison_result == True:\n",
    "    test2 = 'OK - The original dataset is the similar to the resulting dataset'\n",
    "else:\n",
    "     test2 ='ERROR - The original dataset are NOT found'\n",
    "\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Filter dataset to only include the products from the list provided by the client, and save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_products = df_desc_normalezed_vs_prod_by_prod[~df_desc_normalezed_vs_prod_by_prod['target_names_prod_by_prod'].isnull()]\n",
    "df_other_products = df_desc_normalezed_vs_prod_by_prod[df_desc_normalezed_vs_prod_by_prod['target_names_prod_by_prod'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>description</th>\n",
       "      <th>order_date</th>\n",
       "      <th>section</th>\n",
       "      <th>store</th>\n",
       "      <th>units_ordered</th>\n",
       "      <th>desc_normalized</th>\n",
       "      <th>target_names_prod_by_prod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>9999.0</td>\n",
       "      <td>TORTELLINI BOLOGNESA</td>\n",
       "      <td>1/1/2008 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>VeUp</td>\n",
       "      <td>0,00</td>\n",
       "      <td>tortellini bolognesa</td>\n",
       "      <td>tortel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>215.0</td>\n",
       "      <td>CROISANNT SOBRASADA PQ COCIDOS</td>\n",
       "      <td>1/1/2008 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>VeUp</td>\n",
       "      <td>2,00</td>\n",
       "      <td>croissant sobrasada px cocido</td>\n",
       "      <td>croissant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>100.0</td>\n",
       "      <td>CROISANTS</td>\n",
       "      <td>1/1/2008 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>VeUp</td>\n",
       "      <td>20,00</td>\n",
       "      <td>croissant</td>\n",
       "      <td>croissant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>198.0</td>\n",
       "      <td>MINI TORTELES-PIEZAS/HORA 5</td>\n",
       "      <td>1/1/2008 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>VeUp</td>\n",
       "      <td>0,00</td>\n",
       "      <td>mini torteles piezas hora 5</td>\n",
       "      <td>tortel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>182.0</td>\n",
       "      <td>PALMERAS DE TRUFA</td>\n",
       "      <td>1/1/2008 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>VeUp</td>\n",
       "      <td>4,00</td>\n",
       "      <td>palmera de trufa</td>\n",
       "      <td>palmera chocolate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     product_id                     description        order_date  section  \\\n",
       "192      9999.0            TORTELLINI BOLOGNESA  1/1/2008 0:00:00        0   \n",
       "201       215.0  CROISANNT SOBRASADA PQ COCIDOS  1/1/2008 0:00:00        0   \n",
       "216       100.0                       CROISANTS  1/1/2008 0:00:00        0   \n",
       "218       198.0     MINI TORTELES-PIEZAS/HORA 5  1/1/2008 0:00:00        0   \n",
       "234       182.0               PALMERAS DE TRUFA  1/1/2008 0:00:00        0   \n",
       "\n",
       "    store units_ordered                desc_normalized  \\\n",
       "192  VeUp          0,00           tortellini bolognesa   \n",
       "201  VeUp          2,00  croissant sobrasada px cocido   \n",
       "216  VeUp         20,00                      croissant   \n",
       "218  VeUp          0,00    mini torteles piezas hora 5   \n",
       "234  VeUp          4,00               palmera de trufa   \n",
       "\n",
       "    target_names_prod_by_prod  \n",
       "192                    tortel  \n",
       "201                 croissant  \n",
       "216                 croissant  \n",
       "218                    tortel  \n",
       "234         palmera chocolate  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target_products_file_name = exit_path + 'filtered_transactions_not_clean.csv' \n",
    "df_target_products.to_csv(df_target_products_file_name, index = False, sep = ';' )\n",
    "df_target_products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>description</th>\n",
       "      <th>order_date</th>\n",
       "      <th>section</th>\n",
       "      <th>store</th>\n",
       "      <th>units_ordered</th>\n",
       "      <th>desc_normalized</th>\n",
       "      <th>target_names_prod_by_prod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9999.0</td>\n",
       "      <td>PECHUGAS AVE RELLENA DE HONGOS Y SETAS</td>\n",
       "      <td>1/1/2008 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>ViUP</td>\n",
       "      <td>0,00</td>\n",
       "      <td>pechuga de rellena de higos y setas</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3268.0</td>\n",
       "      <td>SURTIDO NAVIDAD 1/4 KG</td>\n",
       "      <td>1/1/2008 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>VeUp</td>\n",
       "      <td>0,00</td>\n",
       "      <td>surtido novedad 1 4 g</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3269.0</td>\n",
       "      <td>SURTIDO NAVIDAD DE 1/2 KG</td>\n",
       "      <td>1/1/2008 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>VeUp</td>\n",
       "      <td>0,00</td>\n",
       "      <td>surtido novedad de 1 2 g</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3309.0</td>\n",
       "      <td>ROLLITOS DE SALMON  RELL. DE MUSELINA DE MARISCOS</td>\n",
       "      <td>1/1/2008 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>VeUp</td>\n",
       "      <td>0,00</td>\n",
       "      <td>rollito de salmon del de mouselina de marcos</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3318.0</td>\n",
       "      <td>FONDOS DE ALCACHOFAS RELLENAS DE MARISCO</td>\n",
       "      <td>1/1/2008 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>VeUp</td>\n",
       "      <td>0,00</td>\n",
       "      <td>todos de alcachofas rellenos de mario</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id                                        description  \\\n",
       "0      9999.0             PECHUGAS AVE RELLENA DE HONGOS Y SETAS   \n",
       "1      3268.0                             SURTIDO NAVIDAD 1/4 KG   \n",
       "2      3269.0                          SURTIDO NAVIDAD DE 1/2 KG   \n",
       "3      3309.0  ROLLITOS DE SALMON  RELL. DE MUSELINA DE MARISCOS   \n",
       "4      3318.0           FONDOS DE ALCACHOFAS RELLENAS DE MARISCO   \n",
       "\n",
       "         order_date  section store units_ordered  \\\n",
       "0  1/1/2008 0:00:00        0  ViUP          0,00   \n",
       "1  1/1/2008 0:00:00        0  VeUp          0,00   \n",
       "2  1/1/2008 0:00:00        0  VeUp          0,00   \n",
       "3  1/1/2008 0:00:00        0  VeUp          0,00   \n",
       "4  1/1/2008 0:00:00        0  VeUp          0,00   \n",
       "\n",
       "                                desc_normalized target_names_prod_by_prod  \n",
       "0           pechuga de rellena de higos y setas                       NaN  \n",
       "1                         surtido novedad 1 4 g                       NaN  \n",
       "2                      surtido novedad de 1 2 g                       NaN  \n",
       "3  rollito de salmon del de mouselina de marcos                       NaN  \n",
       "4         todos de alcachofas rellenos de mario                       NaN  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfiltered_products_file_name = exit_path + 'unfiltered_transactions.csv' \n",
    "df_other_products.to_csv(unfiltered_products_file_name, index = False, sep = ';' )\n",
    "df_other_products.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERROR CONTROL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK - 'df' has the same size as 'df_with_normalized_descriptions' \n",
      "OK - 'df_with_normalized_descriptions' has the same size as 'df_with_normalized_descriptions' \n",
      "OK - The original dataset is the similar to the resulting dataset\n"
     ]
    }
   ],
   "source": [
    "print(test0)\n",
    "print(test1)\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kschool-final-project",
   "language": "python",
   "name": "kschool-final-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSACTIONS TO PARTIAL RESULTS:\n",
    "\n",
    "This script intends to make an exploratory data analysis on the partially-treated data to jump to our first conclussions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. IMPORTING PACKAGES AND THE INFORMATION:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages:\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the search path of the file, the name and the separator:\n",
    "\n",
    "file_path = \"../../data/01_raw/\"\n",
    "file_name = \"b2-transactions.csv\"\n",
    "\n",
    "sep=\";\"\n",
    "\n",
    "\n",
    "# Provisional file:\n",
    "\n",
    "total_sales_results_per_id=\"total_sales_results_per_id.csv\"\n",
    "total_sales_results_per_id_and_store=\"total_sales_results_per_id_and_store.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we import the file and storing it in df: \n",
    "# (at first, we only import several thousand rows)\n",
    "\n",
    "df=pd.read_csv(file_path+file_name, nrows=1000000, sep=sep)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. CHECKING FOR NULL VALUES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if is there any null values:\n",
    "\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also checking the na values:\n",
    "\n",
    "df.isna().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can construct a vector for selecting those rows that have any missing data:\n",
    "\n",
    "# First we create a boleean array that tells us wether the row has missing data:\n",
    "\n",
    "missing_data_check=False\n",
    "for column in df.columns:\n",
    "    missing_data_check = missing_data_check |df[column].isnull()\n",
    "    \n",
    "# We can now slice the column to get only those columns that have missing data:\n",
    "\n",
    "array_of_missing_values=missing_data_check[missing_data_check==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now check the lenght of this resulting vector:\n",
    "\n",
    "len(array_of_missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After checking that the % of the missing data rows is despicable (for 1000000 rows we get 1056, or 0.1%), we decide to drop them:\n",
    "\n",
    "df.dropna(how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop the 'Unnamed: 0' column, due to it seems to be an old index made column, and its information is redundant:\n",
    "\n",
    "df=df.drop('Unnamed: 0', axis=1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. FIRST EVALUATIONS OF THE DATA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Checking the number of different ids and plotting their count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Taking a look of how many different ids exist and their totals (in our 10.000 rows, of course):\n",
    "\n",
    "prod_and_num_trans=df.groupby('product_id').count()['description'].sort_values(ascending=False)\n",
    "prod_and_num_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to take a look for checking how many different values exist in our df:\n",
    "\n",
    "len(prod_and_num_trans)\n",
    "\n",
    "# The result is that in the first 100.000 rows we have 1283 unique ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_sellers_list=prod_and_num_trans.iloc[0:50]\n",
    "best_sellers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=[str(x) for x in best_sellers_list.index]\n",
    "plt.bar(names, height=best_sellers_list.values);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Adding all the orders for each id and getting first totals and sells share:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1. First problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We face our first problem here. The information on units oredered is not of type integer, as we would like to have it, but\n",
    "it is a string. We have to convert it appropriately before going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['units_ordered'][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check on the different positions the comma might be at:\n",
    "\n",
    "comma_positions=df['units_ordered'].str.find(\",\")\n",
    "comma_positions.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The 'units_ordered' column is a string, that cannot be easily converted to integer because\n",
    "# their numbers are in continental format (\",\" instead of \".\" for decimals).\n",
    "\n",
    "# So, we decide to separate the string by the comma, take the first partition and store it in the df as an integer (long, in\n",
    "# provision of numbers in the order of magnitude of the limit of the standard 'int' ~ 31500):\n",
    "\n",
    "df['units_ordered_numeric']=df['units_ordered'].str.split(\",\").str[0].astype(dtype='long')\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2. Getting the totals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take a quick glance to the results to check that everything is fine:\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Proceeding to check the products:\n",
    "\n",
    "# We want to group by id and description no. What we want to check now is:\n",
    "\n",
    "    # Wheter there are ids assigned to several product or there are not\n",
    "    # If an id is assigned to several product, we want to check if there is a logical relationship among those products\n",
    "    # The quantities of the products bought along the lines we have selected\n",
    "\n",
    "totals_by_id_description=df.groupby(['product_id', 'description'], as_index=False).sum().sort_values('units_ordered_numeric', ascending=False)\n",
    "totals_by_id_description.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sales=totals_by_id_description.sum()[3]\n",
    "total_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.3. Getting total shares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_by_id_description['sells_share']=totals_by_id_description['units_ordered_numeric']/total_sales\n",
    "\n",
    "totals_by_id_description.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.3. Second problem:\n",
    "\n",
    "We have a slight problem with the data, which is the relation id-description is not unique, as e can see below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "totals_by_id_description[totals_by_id_description['product_id']==245].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then, proceed to count and order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An accesory table is created to store the counting for each id, then this table is \n",
    "# adjoined to our main df: totals_by_id_description\n",
    "\n",
    "accesory_table_1=totals_by_id_description.groupby('product_id').count()\n",
    "accesory_table_1.columns=['count', 'count2', 'count3', 'count4']\n",
    "\n",
    "totals_by_id_description.merge(accesory_table_1['count'], on='product_id').sort_values('count', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a product id just forf orders (9999), that has cannot be specified as a unique product.\n",
    "\n",
    "On the other hand, we see that this product id has certain particularities: for instance, there are lots of orders with 0 units\n",
    "ordered, which seems extrange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filter1=(totals_by_id_description['product_id']==9999) &  (totals_by_id_description['units_ordered_numeric']==0)\n",
    "totals_by_id_description[filter1].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'\n",
    "\n",
    "So, lets take a look to the relationship between the id and its description:\n",
    "\n",
    "\n",
    "'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dif_id_description_matches=totals_by_id_description[['product_id','description']].sort_values('product_id', ascending=True)\n",
    "dif_id_description_matches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prods_per_id=dif_id_description_matches.groupby('product_id', as_index=False).count().sort_values('description', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prods_per_id.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppi50=prods_per_id[1:50]\n",
    "\n",
    "names2=[str(x) for x in ppi50['product_id']]\n",
    "\n",
    "plt.bar(names2, ppi50['description']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Keeping these lines just in case:\n",
    "\n",
    "# searching_for_unique=totals_by_id_description[['product_id','description']]\n",
    "# searching_for_unique['joined_cols']=searching_for_unique['product_id'].apply(str)+\"/\"+searching_for_unique['description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'\n",
    "\n",
    "\n",
    "These results make us think again the groupby used.\n",
    "\n",
    "Is it, perhaps, more useful to group the products just by id?\n",
    "\n",
    "\n",
    "\n",
    "'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accesory_table2=df.groupby(['product_id'], as_index=False).first()[['product_id','description']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_by_id=df.groupby(['product_id'], as_index=False).agg(sum('units_oredered_numeric').alias('total'), first('description'), count('description').alias('num_rows')).sort_values('total', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1={'units_ordered_numeric':'sum','description':'first','units_ordered':'count'}\n",
    "\n",
    "totals_by_id=df.groupby(['product_id'], as_index=False).agg(dict1).sort_values('units_ordered_numeric', ascending=False)\n",
    "\n",
    "list1=['product_id', 'total_orders', 'description', 'number_of_different_names']\n",
    "\n",
    "totals_by_id.columns=list1\n",
    "\n",
    "totals_by_id.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1={'units_ordered_numeric':'sum','description':'first','units_ordered':'count'}\n",
    "\n",
    "totals_by_id_and_store=df.groupby(['product_id', 'store'], as_index=False).agg(dict1).sort_values(['store','units_ordered_numeric'], ascending=False)\n",
    "\n",
    "list2=['product_id', 'store', 'total_orders', 'description', 'number_of_different_names']\n",
    "\n",
    "totals_by_id_and_store.columns=list2\n",
    "\n",
    "totals_by_id_and_store.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. ENDING:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all this process, we have learned a few things:\n",
    "\n",
    "-We have data related to orders of several products and stores. The products are marked by an id and a description.\n",
    "\n",
    "-Our data has null values, but very few, and we have then decided to discard them.\n",
    "\n",
    "-Our data has not a solid relationship between id and description of the products. In general, an id is assigned to many \"similar\" products. In a few cases, it has been noticed that an id is given to two disimilar products (we are assuming that this is due to human mistake).\n",
    "\n",
    "-Also, there is an id (9999) that, as it is used for direct orders from customers, is assigned to a lot of different products. We could try to reassign this products by its description to their other suitable id, or disregard the whole id. As we were asked not to take into account the direct online orders from customers, the solution should be to not use this id.\n",
    "\n",
    "-Appart from the stated, id seems a better indicator than description, for grouping the aforementioned products.\n",
    "\n",
    "-Some additional checks should be done to adress the suitability of the id as indicator of the product. Specifically, it would be interesting to check the behaviour of id against a manual filter based on some keywords.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking into consideration that we have operated this script over one million lines, the results in terms of comparison of the sales of the different products should be relatively reliable.\n",
    "\n",
    "In this spirit is why we export the two last dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now store these results in a csv, for sending, if it is convenient \n",
    "# (taking into account that these are not results obtained on the total of the information given):\n",
    "\n",
    "totals_by_id.to_csv(file_path+total_sales_results_per_id, sep=sep)\n",
    "totals_by_id_and_store.to_csv(file_path+total_sales_results_per_id_and_store, sep=sep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
